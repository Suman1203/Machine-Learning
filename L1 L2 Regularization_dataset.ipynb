{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c033c3-477a-432c-8f20-ea14f91c8a0d",
   "metadata": {},
   "source": [
    "L1 and L2 regularization are techniques used to prevent overfitting in machine learning models, particularly in linear models like linear regression and logistic regression.\n",
    "\n",
    "**L1 Regularization (Lasso Regression):**\n",
    "\n",
    "- L1 regularization adds a penalty term to the cost function that is proportional to the absolute value of the coefficients of the features. \n",
    "\n",
    "- It penalizes the sum of the absolute values of the model parameters.\n",
    "\n",
    "- λ is the regularization strength hyperparameter, controlling how much the regularization term should affect the overall loss function. Larger values of \n",
    "\n",
    "- λ result in more aggressive regularization.\n",
    "\n",
    "**L1 regularization tends to produce sparse models, meaning it encourages some of the coefficients to be exactly zero, effectively performing feature selection.**\n",
    "\n",
    "**L2 Regularization (Ridge Regression):**\n",
    "\n",
    "- L2 regularization adds a penalty term to the cost function that is proportional to the square of the coefficients of the features. \n",
    "\n",
    "- It penalizes the sum of the squares of the model parameters.\n",
    "\n",
    "- λ is the regularization strength hyperparameter.\n",
    "\n",
    "- L2 regularization encourages smaller but non-zero coefficients. It does not typically result in sparse solutions as L1 regularization does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41e4c48-b719-4fa2-8c4b-7dc07ccd12a4",
   "metadata": {},
   "source": [
    "<img id=\"dimg_391\" src=\"data:image/png;base64,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\" class=\"YQ4gaf\" height=\"173\" style=\"object-position:0% 42%\" width=\"291\" alt=\"How to Apply L1 and L2 Regularization ...\" data-csiid=\"25\" data-atf=\"1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98639cc-ac1f-4deb-ba1e-50d713492254",
   "metadata": {},
   "source": [
    "The choice between L1 and L2 regularization depends on the specific problem and the desired properties of the resulting model:\n",
    "\n",
    "L1 regularization is often preferred when feature selection is desired or when there are a large number of irrelevant features in the dataset.\n",
    "    \n",
    "L2 regularization is generally more stable and is preferred when feature selection is not a primary concern.\n",
    "    \n",
    "Both L1 and L2 regularization techniques are used to prevent overfitting by penalizing large coefficient values, encouraging simpler models that generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1187f19-b7e5-4b01-972b-bb5f6fe84df0",
   "metadata": {},
   "source": [
    "**Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4d723e-c846-442a-95e5-9538ddda3d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.49867516307458865\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\omkar\\\\OneDrive\\\\Documents\\\\Data science\\\\Naresh IT\\\\Naresh IT\\\\Datafiles\\\\winequality_red.csv\")\n",
    "X=df.drop('quality',axis=1)\n",
    "y=df['quality']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Lasso regression (L1 regularization)\n",
    "lasso_reg = Lasso(alpha=0.1)  # Regularization strength (alpha) is set to 0.1\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso_reg.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6766a-19ed-4282-adbf-fb849b4fd954",
   "metadata": {},
   "source": [
    "**Ridge Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b1bf06-540b-406d-94a5-05111eca1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.39079450472195915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\omkar\\\\OneDrive\\\\Documents\\\\Data science\\\\Naresh IT\\\\Naresh IT\\\\Datafiles\\\\winequality_red.csv\")\n",
    "X=df.drop('quality',axis=1)\n",
    "y=df['quality']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Lasso regression (L1 regularization)\n",
    "ridge_reg = Ridge(alpha=0.1)  # Regularization strength (alpha) is set to 0.1\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_reg.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981126e9-f84a-4e92-a1b6-76b3d69bde96",
   "metadata": {},
   "source": [
    "## Elastic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26b2ca-c72a-48dd-b325-efd60a2af32c",
   "metadata": {},
   "source": [
    "Elastic Net regression is a regularization technique that combines both L1 and L2 regularization penalties in order to achieve a balance between the benefits of each.\n",
    "\n",
    "In Elastic Net, the cost function is a combination of the L1 and L2 regularization terms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb7232-2713-4401-87f9-d4e242d088f4",
   "metadata": {},
   "source": [
    "Alpha is the regularization strength parameter (similar to lambda in the equation above), controlling the overall strength of the regularization. \n",
    "\n",
    "The l1_ratio parameter controls the balance between the L1 and L2 penalties. \n",
    "\n",
    "A l1_ratio of 1 corresponds to Lasso regression, and a l1_ratio of 0 corresponds to Ridge regression. \n",
    "\n",
    "Adjusting these hyperparameters allows you to fine-tune the Elastic Net regression model to your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c859991-3b28-4296-9b07-bb9bf5202559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4880295981151681\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\omkar\\\\OneDrive\\\\Documents\\\\Data science\\\\Naresh IT\\\\Naresh IT\\\\Datafiles\\\\winequality_red.csv\")\n",
    "X=df.drop('quality',axis=1)\n",
    "y=df['quality']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the Elastic Net model\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio controls the balance between L1 and L2 penalties\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = elastic_net.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bbe289b-e445-497a-b5fc-e23415ded8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-sqaure: 0.25321312561465037\n",
      "MSE: 0.4880295981151681\n",
      "RMSE: 0.6985911523310098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "R2=r2_score(y_test,y_pred)\n",
    "MSE=mean_squared_error(y_test,y_pred)\n",
    "#MSE**(1/2)\n",
    "RMSE=np.sqrt(MSE)\n",
    "#accuracy_score(y_test,y_predictions) # it is a regression tech\n",
    "print(\"R-sqaure:\",R2)\n",
    "print(\"MSE:\",MSE)\n",
    "print(\"RMSE:\",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2d85f-bdad-4d04-8353-572e6fc9fb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
